{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ff_resnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9cac0b3e04b64b93a1606d4be192d30c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_68b9978cbc4741be9306d0aaa5eedab8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f02bab36c0604a70bde06ad4c7197838",
              "IPY_MODEL_f239fa05e79e4d178709788bb796c473"
            ]
          }
        },
        "68b9978cbc4741be9306d0aaa5eedab8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f02bab36c0604a70bde06ad4c7197838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a8b784f833c0490fa681bb049992a4c9",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1a02e19c31d641248d4d58bb602463d4"
          }
        },
        "f239fa05e79e4d178709788bb796c473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_575e1e5e81e14a22bb70da1658372332",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [9:22:25&lt;00:00, 3.04kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e476a3abad474797adb6b3597a65e91e"
          }
        },
        "a8b784f833c0490fa681bb049992a4c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1a02e19c31d641248d4d58bb602463d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "575e1e5e81e14a22bb70da1658372332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e476a3abad474797adb6b3597a65e91e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rytaylor/FairFace/blob/resnext/ff_resnet_original.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNjVNQ5uFdTp"
      },
      "source": [
        "Start by mounting the drive location to get the CSVs & unzip the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBMXMyWKRgjb",
        "outputId": "5205188a-472a-4925-c312-9ecf8aea8ef8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUf07rKSFlNe"
      },
      "source": [
        "Then read these locations into memory, extracting the zip to a temporary local location to be read as needed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "-ew-HKOaR2ba",
        "outputId": "d991b9fc-971e-4058-b1bf-32bb478c5d2e"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio as io\n",
        "\n",
        "img_zip_path = '/content/drive/My Drive/FairFace-Colab/fairface-img-margin025-trainval.zip'\n",
        "zip_ref = zipfile.ZipFile(img_zip_path, 'r')\n",
        "zip_ref.extractall('/tmp/model_data')\n",
        "zip_ref.close()\n",
        "\n",
        "#print(io.imread('/fairface-img-margin025-trainval/train/1.jpg'))\n",
        "\n",
        "train_data_path = '/content/drive/My Drive/FairFace-Colab/fairface_label_train.csv'\n",
        "train_data_df = pd.read_csv(train_data_path)\n",
        "\n",
        "val_data_path = '/content/drive/My Drive/FairFace-Colab/fairface_label_val.csv'\n",
        "val_data_df = pd.read_csv(train_data_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>race</th>\n",
              "      <th>service_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train/1.jpg</td>\n",
              "      <td>50-59</td>\n",
              "      <td>Male</td>\n",
              "      <td>East Asian</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train/2.jpg</td>\n",
              "      <td>30-39</td>\n",
              "      <td>Female</td>\n",
              "      <td>Indian</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train/3.jpg</td>\n",
              "      <td>3-9</td>\n",
              "      <td>Female</td>\n",
              "      <td>Black</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train/4.jpg</td>\n",
              "      <td>20-29</td>\n",
              "      <td>Female</td>\n",
              "      <td>Indian</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train/5.jpg</td>\n",
              "      <td>20-29</td>\n",
              "      <td>Female</td>\n",
              "      <td>Indian</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          file    age  gender        race  service_test\n",
              "0  train/1.jpg  50-59    Male  East Asian          True\n",
              "1  train/2.jpg  30-39  Female      Indian         False\n",
              "2  train/3.jpg    3-9  Female       Black         False\n",
              "3  train/4.jpg  20-29  Female      Indian          True\n",
              "4  train/5.jpg  20-29  Female      Indian          True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20JI73f7WgLH"
      },
      "source": [
        "Now the dataset class is defined, which allows the model to grab the training & validation data and implements the label tensor logic:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GH8pUhuUVxT"
      },
      "source": [
        "import torchvision\n",
        "from glob import glob\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from torch.optim import lr_scheduler\n",
        "from torch import optim\n",
        "from torchvision.utils import make_grid\n",
        "import time\n",
        "from torch.utils.data import Dataset\n",
        "%matplotlib inline\n",
        "\n",
        "class FaceImageDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, csv_path, rootdir, transform=None):\n",
        "        self.csv_file = pd.read_csv(csv_path)\n",
        "        self.rootdir = rootdir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv_file)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        \n",
        "        img_name = os.path.join(self.rootdir, self.csv_file.iloc[index, 0])\n",
        "        image = io.imread(img_name)\n",
        "\n",
        "        age = self.csv_file.iloc[index, 1]\n",
        "        gender = self.csv_file.iloc[index, 2]\n",
        "        race = self.csv_file.iloc[index, 3]\n",
        "        service_test = self.csv_file.iloc[index, 4]\n",
        "\n",
        "        if(self.transform):\n",
        "            image = self.transform(image)\n",
        "\n",
        "        age_list = ['0-2', '3-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60-69', 'more than 70']\n",
        "        race_list = ['White', 'Black', 'Latino_Hispanic', 'East Asian', 'Southeast Asian', 'Indian', 'Middle Eastern']\n",
        "        gender_list = ['Male', 'Female']\n",
        "\n",
        "        service_test = 1 if service_test == True else 0\n",
        "\n",
        "        label = torch.zeros([18])\n",
        "        label[age_list.index(age)] = 1\n",
        "        label[9+race_list.index(race)] = 1\n",
        "        label[16+gender_list.index(gender)] = 1\n",
        "\n",
        "        label = torch.as_tensor(label, dtype=torch.float64)\n",
        "\n",
        "        sample = {'image': image, 'label': label}\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m_ZtbPpF48u"
      },
      "source": [
        "Two datasets are defined, training and validation, with a set of transforms to match what a pretrained ResNet expects:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzqW-NvKWitL"
      },
      "source": [
        "data_transforms = torchvision.transforms.Compose([transforms.ToPILImage(),\n",
        "                                                  transforms.Resize((224, 224)),\n",
        "                                                  transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
        "                                                  transforms.ToTensor()])\n",
        "train_dl = FaceImageDataset(csv_path=train_data_path, rootdir='/tmp/model_data/', transform=data_transforms)\n",
        "valid_dl = FaceImageDataset(csv_path=val_data_path, rootdir='/tmp/model_data/', transform=data_transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUszg-nGGCpn"
      },
      "source": [
        "DataLoaders are created to wrap the above dataset classes for use by the model in training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ73HgcEaBhD"
      },
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_dl, shuffle = False, batch_size = 64, num_workers = 3)\n",
        "val_dataloader = torch.utils.data.DataLoader(valid_dl, shuffle = True, batch_size = 64, num_workers = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmiBE--UGHSL"
      },
      "source": [
        "Here, we define the model; it is a ResNet50 with adapted fully-connected layers and sigmoid reshaping of the final linear layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCF_XF0GT9jZ"
      },
      "source": [
        "class ResnetModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ResnetModel, self).__init__()\n",
        "    self.model = torchvision.models.resnet50(pretrained=True)\n",
        "    self.model.fc = nn.Sequential(nn.Linear(2048, 512),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout(0.2),\n",
        "                                    nn.Linear(512, 18))\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    #\n",
        "    self.cuda()\n",
        "    #self.model.cuda()\n",
        "  def forward(self, x):\n",
        "    x = self.model(x)\n",
        "    return self.sigmoid(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z7sG2zrGaMN"
      },
      "source": [
        "The below function is a helper to get the accuracy of a given prediction relative to the original data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkrVnVMLaJ0s"
      },
      "source": [
        "def pred_acc(original, predicted):\n",
        "  return torch.round(predicted).eq(original).sum().numpy()/len(original) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCTg701zGjZ8"
      },
      "source": [
        "Now, the training loop is defined with binary cross-entropy loss for multilabel classifying as well as a stochastic gradient descent optimizer with default parameters; this is pretty much boilerplate model fitting for pytorch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm_inNobaO5k"
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "def fit_model(epochs, model, dataloader, phase = 'training', volatile = False):\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9, weight_decay = 1e-5)\n",
        "    \n",
        "    pprint(\"Epoch: {}\".format(epochs))\n",
        "    if phase == 'training':\n",
        "        model.train()\n",
        "        \n",
        "    if phase == 'validation':\n",
        "        model.eval()\n",
        "        volatile = True\n",
        "        \n",
        "    running_loss = []\n",
        "    running_accuracy = []\n",
        "    for i, data in enumerate(dataloader):\n",
        "        \n",
        "        inputs, target = data['image'].cuda(), data['label'].float().cuda()\n",
        "        \n",
        "        inputs, target = Variable(inputs), Variable(target)\n",
        "        \n",
        "        if phase == 'training':\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "        ops = model(inputs)\n",
        "        accuracy_out = []\n",
        "\n",
        "        for i, d in enumerate(ops, 0):\n",
        "          accuracy = pred_acc(torch.Tensor.cpu(target[i]), torch.Tensor.cpu(d))\n",
        "          accuracy_out.append(accuracy)\n",
        "          \n",
        "        loss = criterion(ops, target)\n",
        "                        \n",
        "        running_loss.append(loss.item())\n",
        "        running_accuracy.append(np.asarray(accuracy_out).mean())\n",
        "        \n",
        "        if phase == 'training':\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "              \n",
        "    total_batch_loss = np.asarray(running_loss).mean()\n",
        "    total_batch_accuracy = np.asarray(running_accuracy).mean()\n",
        "    \n",
        "    pprint(\"{} loss is {} \".format(phase,total_batch_loss))\n",
        "    pprint(\"{} accuracy is {} \".format(phase, total_batch_accuracy))\n",
        "    \n",
        "    return total_batch_loss, total_batch_accuracy\n",
        "\n",
        "def check_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "is_cuda = check_cuda()\n",
        "model = ResnetModel()\n",
        "if is_cuda:\n",
        "    model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEBL50jmHmIF"
      },
      "source": [
        "And below is the call to fit_model for the training and validation items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trA3GzSmV_Sq",
        "outputId": "12cf4c86-453b-4497-d81f-e5c95db20006"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import imageio as io\n",
        "import time\n",
        "\n",
        "train_loss = []\n",
        "train_accuracy = []\n",
        "val_loss = []\n",
        "val_accuracy = []\n",
        "for i in tqdm(range(1, 6)):\n",
        "  train_l, train_a = fit_model(i, model, train_dataloader)\n",
        "  val_l, val_a = fit_model(i, model, val_dataloader, phase = 'validation')\n",
        "  train_loss.append(trn_l); train_accuracy.append(trn_a)\n",
        "  val_loss.append(val_l); val_accuracy.append(val_a)\n",
        "  torch.save(model, \"drive/My Drive/FairFace-Colab/model_res_{}\".format(i))\n",
        "torch.save(model, \"drive/My Drive/FairFace-Colab/model_res_final\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 1'\n",
            "'training loss is 0.3200726832841341 '\n",
            "'training accuracy is 0.8680049643428008 '\n",
            "'Epoch: 1'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 25%|â–ˆâ–ˆâ–Œ       | 1/4 [06:09<18:29, 369.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'validation loss is 0.30736675279853987 '\n",
            "'validation accuracy is 0.8726591646390915 '\n",
            "'Epoch: 2'\n",
            "'training loss is 0.2943079737401545 '\n",
            "'training accuracy is 0.8771498063445223 '\n",
            "'Epoch: 2'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [12:11<12:14, 367.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'validation loss is 0.2904757005237315 '\n",
            "'validation accuracy is 0.8788381995133819 '\n",
            "'Epoch: 3'\n",
            "'training loss is 0.2742673765163015 '\n",
            "'training accuracy is 0.8844848917988444 '\n",
            "'Epoch: 3'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [18:14<06:06, 366.21s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'validation loss is 0.2818192746734967 '\n",
            "'validation accuracy is 0.8822566909975669 '\n",
            "'Epoch: 4'\n",
            "'training loss is 0.258962304026647 '\n",
            "'training accuracy is 0.8902913541538588 '\n",
            "'Epoch: 4'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [24:19<00:00, 364.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'validation loss is 0.28163704589335586 '\n",
            "'validation accuracy is 0.8829643146796431 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JdQ-9riH6--"
      },
      "source": [
        "Here is an alternate form allowing for retraining of a model from a saved compressed model file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "OqwKiE1b2N86",
        "outputId": "8f92e0b7-0977-4c45-8c1a-b3d86cba7864"
      },
      "source": [
        "model = MultiClassifier()\n",
        "model = torch.load('drive/My Drive/FairFace-Colab/model6')\n",
        "from tqdm import tqdm\n",
        "import imageio as io\n",
        "import time\n",
        "\n",
        "if is_cuda:\n",
        "  model.cuda()\n",
        "trn_losses = []; trn_acc = []\n",
        "val_losses = []; val_acc = []\n",
        "for i in tqdm(range(1, 11)):\n",
        "  trn_l, trn_a = fit_model(i, model, train_dataloader)\n",
        "  val_l, val_a = fit_model(i, model, val_dataloader, phase = 'validation')\n",
        "  trn_losses.append(trn_l); trn_acc.append(trn_a)\n",
        "  val_losses.append(val_l); val_acc.append(val_a)\n",
        "torch.save(model, \"drive/My Drive/FairFace-Colab/model7\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e1da5a2a503c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/My Drive/FairFace-Colab/model6'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimageio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MultiClassifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plBBwXB9X9xz"
      },
      "source": [
        "==\n",
        "\n",
        "\n",
        "\n",
        "Below is the full training implementation to run all at once:\n",
        "\n",
        "\n",
        "\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9cac0b3e04b64b93a1606d4be192d30c",
            "68b9978cbc4741be9306d0aaa5eedab8",
            "f02bab36c0604a70bde06ad4c7197838",
            "f239fa05e79e4d178709788bb796c473",
            "a8b784f833c0490fa681bb049992a4c9",
            "1a02e19c31d641248d4d58bb602463d4",
            "575e1e5e81e14a22bb70da1658372332",
            "e476a3abad474797adb6b3597a65e91e"
          ]
        },
        "id": "GzYtJurqX9W8",
        "outputId": "aba21056-ef2e-460b-e835-afa097b9315f"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torchvision\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from torch.optim import lr_scheduler\n",
        "from torch import optim\n",
        "from torchvision.utils import make_grid\n",
        "import time\n",
        "from torch.utils.data import Dataset\n",
        "%matplotlib inline\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio as io\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "rootd = '/content/drive/My Drive/FairFace-Colab/'\n",
        "\n",
        "img_zip_path = os.path.join(rootd,'fairface-img-margin025-trainval.zip')\n",
        "zip_ref = zipfile.ZipFile(img_zip_path, 'r')\n",
        "zip_ref.extractall('/tmp/model_data')\n",
        "zip_ref.close()\n",
        "\n",
        "train_data_path = os.path.join(rootd,'fairface_label_train.csv')\n",
        "train_data_df = pd.read_csv(train_data_path)\n",
        "\n",
        "val_data_path = os.path.join(rootd,'fairface_label_val.csv')\n",
        "val_data_df = pd.read_csv(train_data_path)\n",
        "val_data_df.head()\n",
        "\n",
        "class FaceImageDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, csv_path, rootdir, transform=None):\n",
        "        self.csv_file = pd.read_csv(csv_path)\n",
        "        self.rootdir = rootdir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv_file)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        \n",
        "        img_name = os.path.join(self.rootdir, self.csv_file.iloc[index, 0])\n",
        "        image = io.imread(img_name)\n",
        "\n",
        "        age = self.csv_file.iloc[index, 1]\n",
        "        gender = self.csv_file.iloc[index, 2]\n",
        "        race = self.csv_file.iloc[index, 3]\n",
        "        service_test = self.csv_file.iloc[index, 4]\n",
        "\n",
        "        if(self.transform):\n",
        "            image = self.transform(image)\n",
        "\n",
        "        age_list = ['0-2', '3-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60-69', 'more than 70']\n",
        "        race_list = ['White', 'Black', 'Latino_Hispanic', 'East Asian', 'Southeast Asian', 'Indian', 'Middle Eastern']\n",
        "        gender_list = ['Male', 'Female']\n",
        "\n",
        "        service_test = 1 if service_test == True else 0\n",
        "\n",
        "        label = torch.zeros([18])\n",
        "        label[age_list.index(age)] = 1\n",
        "        label[9+race_list.index(race)] = 1\n",
        "        label[16+gender_list.index(gender)] = 1\n",
        "\n",
        "        label = torch.as_tensor(label, dtype=torch.float64)\n",
        "\n",
        "        sample = {'image': image, 'label': label}\n",
        "\n",
        "        return sample\n",
        "\n",
        "data_transforms = torchvision.transforms.Compose([transforms.ToPILImage(),\n",
        "                                                  transforms.Resize((224, 224)),\n",
        "                                                  transforms.ToTensor(),\n",
        "                                                  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                                                  ])\n",
        "train_dl = FaceImageDataset(csv_path=train_data_path, rootdir='/tmp/model_data/', transform=data_transforms)\n",
        "valid_dl = FaceImageDataset(csv_path=val_data_path, rootdir='/tmp/model_data/', transform=data_transforms)\n",
        "\n",
        "#print(train_dl.__getitem__(0))\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dl, shuffle = False, batch_size = 16, num_workers = 3)\n",
        "val_dataloader = torch.utils.data.DataLoader(valid_dl, shuffle = True, batch_size = 16, num_workers = 3)\n",
        "\n",
        "class ResnetModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ResnetModel, self).__init__()\n",
        "    self.model = torchvision.models.resnet50(pretrained=True)\n",
        "    self.model.fc = nn.Sequential(nn.Linear(2048, 512),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout(0.2),\n",
        "                                    nn.Linear(512, 18))\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    #\n",
        "    self.cuda()\n",
        "    #self.model.cuda()\n",
        "  def forward(self, x):\n",
        "    x = self.model(x)\n",
        "    return self.sigmoid(x)\n",
        "\n",
        "def pred_acc(original, predicted):\n",
        "  return torch.round(predicted).eq(original).sum().numpy()/len(original) \n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "def fit_model(epochs, model, dataloader, phase = 'training', volatile = False):\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9, weight_decay = 1e-5)\n",
        "    \n",
        "    pprint(\"Epoch: {}\".format(epochs))\n",
        "    if phase == 'training':\n",
        "        model.train()\n",
        "        \n",
        "    if phase == 'validataion':\n",
        "        model.eval()\n",
        "        volatile = True\n",
        "        \n",
        "    running_loss = []\n",
        "    running_acc = []\n",
        "    b = 0\n",
        "    for i, data in enumerate(dataloader):\n",
        "        \n",
        "        inputs, target = data['image'].cuda(), data['label'].float().cuda()\n",
        "        #add .cuda() to above\n",
        "        \n",
        "        inputs, target = Variable(inputs), Variable(target)\n",
        "        \n",
        "        if phase == 'training':\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "        ops = model(inputs)\n",
        "        acc_ = []\n",
        "\n",
        "        for i, d in enumerate(ops, 0):\n",
        "          acc = pred_acc(torch.Tensor.cpu(target[i]), torch.Tensor.cpu(d))\n",
        "          acc_.append(acc)\n",
        "          \n",
        "        loss = criterion(ops, target)\n",
        "                        \n",
        "        running_loss.append(loss.item())\n",
        "        running_acc.append(np.asarray(acc_).mean())\n",
        "        b += 1\n",
        "        \n",
        "        if phase == 'training':\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "              \n",
        "    total_batch_loss = np.asarray(running_loss).mean()\n",
        "    total_batch_acc = np.asarray(running_acc).mean()\n",
        "    \n",
        "    pprint(\"{} loss is {} \".format(phase,total_batch_loss))\n",
        "    pprint(\"{} accuracy is {} \".format(phase, total_batch_acc))\n",
        "    \n",
        "    return total_batch_loss, total_batch_acc\n",
        "\n",
        "def check_cuda():\n",
        "    _cuda = False\n",
        "    if torch.cuda.is_available():\n",
        "        _cuda = True\n",
        "    return _cuda\n",
        "\n",
        "is_cuda = check_cuda()\n",
        "\n",
        "model = ResnetModel()\n",
        "###\n",
        "model = torch.load(os.path.join(rootd,'model_res2_2'))\n",
        "\n",
        "#if is_cuda:\n",
        "#    model.cuda()\n",
        "\n",
        "from tqdm import tqdm\n",
        "import imageio as io\n",
        "import time\n",
        "\n",
        "\n",
        "trn_losses = []; trn_acc = []\n",
        "val_losses = []; val_acc = []\n",
        "for i in tqdm(range(1, 31)):\n",
        "  trn_l, trn_a = fit_model(i, model, train_dataloader)\n",
        "  val_l, val_a = fit_model(i, model, val_dataloader, phase = 'validation')\n",
        "  trn_losses.append(trn_l); trn_acc.append(trn_a)\n",
        "  val_losses.append(val_l); val_acc.append(val_a)\n",
        "  torch.save(model, os.path.join(rootd, 'model_res3_{}'.format(i)))\n",
        "torch.save(model, os.path.join(rootd, 'model_res3_final'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9cac0b3e04b64b93a1606d4be192d30c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/30 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 1'\n",
            "'training loss is 0.1212176025409852 '\n",
            "'training accuracy is 0.9483092288618385 '\n",
            "'Epoch: 1'\n",
            "'validation loss is 0.2722037834733942 '\n",
            "'validation accuracy is 0.8964517437145174 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  3%|â–Ž         | 1/30 [12:43<6:08:56, 763.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 2'\n",
            "'training loss is 0.11192488822328873 '\n",
            "'training accuracy is 0.9530186944137056 '\n",
            "'Epoch: 2'\n",
            "'validation loss is 0.2885114051347231 '\n",
            "'validation accuracy is 0.8953649635036496 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|â–‹         | 2/30 [25:27<5:56:24, 763.72s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 3'\n",
            "'training loss is 0.10345745807543139 '\n",
            "'training accuracy is 0.9568930847575721 '\n",
            "'Epoch: 3'\n",
            "'validation loss is 0.2941124247590991 '\n",
            "'validation accuracy is 0.8950344687753445 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|â–ˆ         | 3/30 [38:12<5:43:50, 764.07s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 4'\n",
            "'training loss is 0.09558235128674339 '\n",
            "'training accuracy is 0.960305109840567 '\n",
            "'Epoch: 4'\n",
            "'validation loss is 0.31040530655070814 '\n",
            "'validation accuracy is 0.893007907542579 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 13%|â–ˆâ–Ž        | 4/30 [50:58<5:31:21, 764.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 5'\n",
            "'training loss is 0.08855024246654906 '\n",
            "'training accuracy is 0.9638298444608384 '\n",
            "'Epoch: 5'\n",
            "'validation loss is 0.3197712940021153 '\n",
            "'validation accuracy is 0.892654095701541 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 17%|â–ˆâ–‹        | 5/30 [1:03:52<5:19:41, 767.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 6'\n",
            "'training loss is 0.08061794248179058 '\n",
            "'training accuracy is 0.9672668449526619 '\n",
            "'Epoch: 6'\n",
            "'validation loss is 0.32568501972586567 '\n",
            "'validation accuracy is 0.8953132603406326 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|â–ˆâ–ˆ        | 6/30 [1:16:40<5:06:59, 767.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 7'\n",
            "'training loss is 0.07458643292632382 '\n",
            "'training accuracy is 0.9699622679208163 '\n",
            "'Epoch: 7'\n",
            "'validation loss is 0.34677573409828827 '\n",
            "'validation accuracy is 0.8929501216545013 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 23%|â–ˆâ–ˆâ–Ž       | 7/30 [1:29:30<4:54:30, 768.30s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 8'\n",
            "'training loss is 0.06822748684294437 '\n",
            "'training accuracy is 0.9727697600311488 '\n",
            "'Epoch: 8'\n",
            "'validation loss is 0.3592700567341199 '\n",
            "'validation accuracy is 0.8951196269261963 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 27%|â–ˆâ–ˆâ–‹       | 8/30 [1:42:13<4:41:09, 766.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 9'\n",
            "'training loss is 0.06207049898260449 '\n",
            "'training accuracy is 0.9753947395385058 '\n",
            "'Epoch: 9'\n",
            "'validation loss is 0.3548149739956334 '\n",
            "'validation accuracy is 0.8952230332522303 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 30%|â–ˆâ–ˆâ–ˆ       | 9/30 [1:54:53<4:27:36, 764.60s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 10'\n",
            "'training loss is 0.056941395578407814 '\n",
            "'training accuracy is 0.9774971566457641 '\n",
            "'Epoch: 10'\n",
            "'validation loss is 0.37710605539979725 '\n",
            "'validation accuracy is 0.8942244525547446 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [2:07:34<4:14:34, 763.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 11'\n",
            "'training loss is 0.05225932219169473 '\n",
            "'training accuracy is 0.9796898694618631 '\n",
            "'Epoch: 11'\n",
            "'validation loss is 0.3786860194737024 '\n",
            "'validation accuracy is 0.8942862935928629 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [2:20:16<4:01:41, 763.24s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 12'\n",
            "'training loss is 0.04790128301923572 '\n",
            "'training accuracy is 0.9816174587073241 '\n",
            "'Epoch: 12'\n",
            "'validation loss is 0.40412014764155785 '\n",
            "'validation accuracy is 0.8936405109489051 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [2:32:59<3:48:53, 762.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 13'\n",
            "'training loss is 0.04474510653861683 '\n",
            "'training accuracy is 0.9827784950202877 '\n",
            "'Epoch: 13'\n",
            "'validation loss is 0.4026525623171869 '\n",
            "'validation accuracy is 0.8958140713706405 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [2:45:46<3:36:30, 764.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 14'\n",
            "'training loss is 0.04154535198877234 '\n",
            "'training accuracy is 0.9841303690725031 '\n",
            "'Epoch: 14'\n",
            "'validation loss is 0.4159525925878191 '\n",
            "'validation accuracy is 0.8942386455798863 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [2:58:34<3:24:05, 765.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 15'\n",
            "'training loss is 0.03754710196349935 '\n",
            "'training accuracy is 0.9857531302512397 '\n",
            "'Epoch: 15'\n",
            "'validation loss is 0.42647508280555696 '\n",
            "'validation accuracy is 0.894073398215734 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [3:11:31<3:12:13, 768.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 16'\n",
            "'training loss is 0.035412228338127356 '\n",
            "'training accuracy is 0.9866945110455346 '\n",
            "'Epoch: 16'\n",
            "'validation loss is 0.4285288960829268 '\n",
            "'validation accuracy is 0.8963219789132197 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [3:24:22<2:59:32, 769.44s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 17'\n",
            "'training loss is 0.031642615731409814 '\n",
            "'training accuracy is 0.9881450059428666 '\n",
            "'Epoch: 17'\n",
            "'validation loss is 0.4367397656188394 '\n",
            "'validation accuracy is 0.8956589618815896 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [3:37:08<2:46:31, 768.55s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 18'\n",
            "'training loss is 0.029371418892180052 '\n",
            "'training accuracy is 0.9891004754293208 '\n",
            "'Epoch: 18'\n",
            "'validation loss is 0.461132883242447 '\n",
            "'validation accuracy is 0.8967609489051095 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [3:49:48<2:33:09, 765.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 19'\n",
            "'training loss is 0.027663047187894647 '\n",
            "'training accuracy is 0.989950920119677 '\n",
            "'Epoch: 19'\n",
            "'validation loss is 0.4711989094741153 '\n",
            "'validation accuracy is 0.8961942416869424 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [4:02:31<2:20:14, 764.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 20'\n",
            "'training loss is 0.026198132486491936 '\n",
            "'training accuracy is 0.9903274724373949 '\n",
            "'Epoch: 20'\n",
            "'validation loss is 0.4670595838205658 '\n",
            "'validation accuracy is 0.8953477291159773 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [4:15:13<2:07:22, 764.26s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 21'\n",
            "'training loss is 0.024265039697676287 '\n",
            "'training accuracy is 0.9911740747571621 '\n",
            "'Epoch: 21'\n",
            "'validation loss is 0.4712120254326911 '\n",
            "'validation accuracy is 0.89838300892133 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [4:28:00<1:54:45, 765.04s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 22'\n",
            "'training loss is 0.021978349855895113 '\n",
            "'training accuracy is 0.9920129923357512 '\n",
            "'Epoch: 22'\n",
            "'validation loss is 0.48864589707694783 '\n",
            "'validation accuracy is 0.8966139497161395 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [4:40:49<1:42:09, 766.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 23'\n",
            "'training loss is 0.0200223637987964 '\n",
            "'training accuracy is 0.9928832892741506 '\n",
            "'Epoch: 23'\n",
            "'validation loss is 0.5012250059277472 '\n",
            "'validation accuracy is 0.8977960259529602 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [4:53:35<1:29:23, 766.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 24'\n",
            "'training loss is 0.019806814665349348 '\n",
            "'training accuracy is 0.9928224517398253 '\n",
            "'Epoch: 24'\n",
            "'validation loss is 0.5104363986610496 '\n",
            "'validation accuracy is 0.8953294809407947 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [5:06:23<1:16:39, 766.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 25'\n",
            "'training loss is 0.018603290543848634 '\n",
            "'training accuracy is 0.9933219599163899 '\n",
            "'Epoch: 25'\n",
            "'validation loss is 0.5115217943478675 '\n",
            "'validation accuracy is 0.8972607461476075 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [5:19:17<1:04:04, 768.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 26'\n",
            "'training loss is 0.01701016411359952 '\n",
            "'training accuracy is 0.9939232909135621 '\n",
            "'Epoch: 26'\n",
            "'validation loss is 0.5076939358110846 '\n",
            "'validation accuracy is 0.8980413625304136 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [5:32:07<51:17, 769.37s/it]  "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 27'\n",
            "'training loss is 0.016594257865923265 '\n",
            "'training accuracy is 0.9940750645518259 '\n",
            "'Epoch: 27'\n",
            "'validation loss is 0.5176738373554536 '\n",
            "'validation accuracy is 0.8962682481751824 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [5:45:00<38:30, 770.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 28'\n",
            "'training loss is 0.015485898355312016 '\n",
            "'training accuracy is 0.9945771343087833 '\n",
            "'Epoch: 28'\n",
            "'validation loss is 0.5270475344501273 '\n",
            "'validation accuracy is 0.8982197891321979 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [5:57:52<25:41, 770.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 29'\n",
            "'training loss is 0.014216043666828415 '\n",
            "'training accuracy is 0.9950599922127955 '\n",
            "'Epoch: 29'\n",
            "'validation loss is 0.5294592761645351 '\n",
            "'validation accuracy is 0.8972810218978101 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [6:10:51<12:53, 773.20s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "'Epoch: 30'\n",
            "'training loss is 0.013827420693724684 '\n",
            "'training accuracy is 0.9951515687118323 '\n",
            "'Epoch: 30'\n",
            "'validation loss is 0.5362720081188382 '\n",
            "'validation accuracy is 0.897875101378751 '\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [6:23:42<00:00, 767.41s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr9OjKDLp6oO"
      },
      "source": [
        "Below is the implementation for predicting the classification for an input image, all at once.\n",
        "\n",
        "\n",
        "*   *detect_face* is an identical implementation from the FairFace predict.py script.\n",
        "*   *predict* is mostly standard pytorch usage, formatted to better display results for each label.\n",
        "\n",
        "*   *get_tensor* implements the same transforms as above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp9XDNTSLssP",
        "outputId": "c438b402-a356-4846-f08a-60aec6f55cec"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torchvision\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from torch.optim import lr_scheduler\n",
        "from torch import optim\n",
        "from torchvision.utils import make_grid\n",
        "import time\n",
        "from torch.utils.data import Dataset\n",
        "%matplotlib inline\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio as io\n",
        "import dlib\n",
        "\n",
        "rootd = '/content/drive/My Drive/FairFace-Colab/'\n",
        "\n",
        "class ResnetModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ResnetModel, self).__init__()\n",
        "    self.model = torchvision.models.resnet50(pretrained=True)\n",
        "    self.model.fc = nn.Sequential(nn.Linear(2048, 512),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout(0.2),\n",
        "                                    nn.Linear(512, 18))\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    #\n",
        "    self.cuda()\n",
        "    #self.model.cuda()\n",
        "  def forward(self, x):\n",
        "    x = self.model(x)\n",
        "    return self.sigmoid(x)\n",
        "\n",
        "def predict(img, label_lst, model):\n",
        "  img = detect_face(img)\n",
        "  tnsr = get_tensor(img)\n",
        "  op = model(tnsr)\n",
        "  op_b = torch.round(op)\n",
        "  op_b_np = torch.Tensor.cpu(op_b).detach().numpy()\n",
        "  preds = np.where(op_b_np == 1)[1]\n",
        "  sigs_op = torch.Tensor.cpu(torch.round((op)*100)).detach().numpy()[0]\n",
        "  o_p = np.argsort(torch.Tensor.cpu(op).detach().numpy())[0][::-1]\n",
        "  age = label_lst[0][np.argmax(sigs_op[:9])]\n",
        "  race = label_lst[1][np.argmax(sigs_op[9:16])]\n",
        "  gender = label_lst[2][np.argmax(sigs_op[16:])]\n",
        "  print(sigs_op)\n",
        "  return [[race, np.max(sigs_op[9:16])], [gender, np.max(sigs_op[16:])], [age, np.max(sigs_op[:9])]]\n",
        "\n",
        "def detect_face(image_path, default_max_size=800,size = 300, padding = 0.25):\n",
        "    cnn_face_detector = dlib.cnn_face_detection_model_v1(os.path.join(rootd, 'dlib_models/mmod_human_face_detector.dat'))\n",
        "    sp = dlib.shape_predictor(os.path.join(rootd, 'dlib_models/shape_predictor_5_face_landmarks.dat'))\n",
        "    base = 2000  # largest width and height\n",
        "    img = dlib.load_rgb_image(image_path)\n",
        "\n",
        "    old_height, old_width, _ = img.shape\n",
        "\n",
        "    if old_width > old_height:\n",
        "        new_width, new_height = default_max_size, int(default_max_size * old_height / old_width)\n",
        "    else:\n",
        "        new_width, new_height =  int(default_max_size * old_width / old_height), default_max_size\n",
        "    img = dlib.resize_image(img, rows=new_height, cols=new_width)\n",
        "\n",
        "    dets = cnn_face_detector(img, 1)\n",
        "    num_faces = len(dets)\n",
        "    if num_faces == 0:\n",
        "        print(\"Sorry, there were no faces found in '{}'\".format(image_path))\n",
        "        return\n",
        "    # Find the 5 face landmarks we need to do the alignment.\n",
        "    faces = dlib.full_object_detections()\n",
        "    for detection in dets:\n",
        "        rect = detection.rect\n",
        "        faces.append(sp(img, rect))\n",
        "    image = dlib.get_face_chips(img, faces, size=size, padding = padding)\n",
        "    return image[0]\n",
        "\n",
        "labels = [['0-2', '3-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60-69', 'more than 70'],\n",
        "          ['White', 'Black', 'Latino_Hispanic', 'East Asian', 'Southeast Asian', 'Indian', 'Middle Eastern'],\n",
        "          ['Male', 'Female']]\n",
        "\n",
        "import imageio as io\n",
        "import numpy as np\n",
        "\n",
        "def get_tensor(img):\n",
        "  tfms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "  ])\n",
        "  return tfms(np.asarray(img)).unsqueeze(0)\n",
        "\n",
        "model_path = os.path.join(rootd, 'model_res3_final')\n",
        "model = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "model = model.eval()\n",
        "predict(os.path.join(rootd, 'test/squid.jpg'), labels, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.  0.  0. 75. 23.  1.  3.  0.  0.  0.  0.  0. 99.  0.  0.  0. 96.  4.]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['East Asian', 99.0], ['Male', 96.0], ['20-29', 75.0]]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq_RQuWvI0Tr"
      },
      "source": [
        "The model was also manually tested on the original set of test_imgs from FairFace, as well as manually inserted images from the Internet. The final model used was trained for 45 epochs, selected for from two previous models. The validation loss (and accuracy) stabilized quickly, but there was volatility in many of the race classifications throughout much of the training when comparing each successive generation. The most firm predictions came from this final model version, with relatively equal performance per individual classification to the human eye. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_BBJCWjaia4"
      },
      "source": [
        "import torch.onnx\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torchvision\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from torch.optim import lr_scheduler\n",
        "from torch import optim\n",
        "from torchvision.utils import make_grid\n",
        "import time\n",
        "from torch.utils.data import Dataset\n",
        "%matplotlib inline\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio as io\n",
        "import dlib\n",
        "\n",
        "class ResnetModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ResnetModel, self).__init__()\n",
        "    self.model = torchvision.models.resnet50(pretrained=True)\n",
        "    self.model.fc = nn.Sequential(nn.Linear(2048, 512),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout(0.2),\n",
        "                                    nn.Linear(512, 18))\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    #\n",
        "    self.cuda()\n",
        "    #self.model.cuda()\n",
        "  def forward(self, x):\n",
        "    x = self.model(x)\n",
        "    return self.sigmoid(x)\n",
        "\n",
        "\n",
        "rootd = '/content/drive/My Drive/FairFace-Colab/'\n",
        "device = torch.device('cpu')\n",
        "model = torch.load(os.path.join(rootd, 'model_res3_final'), map_location=device)\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "model.eval()\n",
        "# trace model with a dummy input\n",
        "traced_model = torch.jit.trace(model, dummy_input)\n",
        "traced_model.save((os.path.join(rootd, 'res3f_traced.pt')))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pipreqs\n",
        "!pip install nbconvert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t1VeoIFjIlW",
        "outputId": "03c471a2-74b0-4b08-d524-01a606683253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pipreqs\n",
            "  Downloading pipreqs-0.4.11-py2.py3-none-any.whl (32 kB)\n",
            "Collecting yarg\n",
            "  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from pipreqs) (0.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from yarg->pipreqs) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (3.0.4)\n",
            "Installing collected packages: yarg, pipreqs\n",
            "Successfully installed pipreqs-0.4.11 yarg-0.1.9\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (5.6.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from nbconvert) (2.6.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert) (4.1.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbconvert) (4.9.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (0.3)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (2.11.3)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (5.1.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert) (0.5.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (5.1.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert) (0.8.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.4->nbconvert) (2.0.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.4->nbconvert) (0.2.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.4->nbconvert) (2.6.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert) (1.15.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert) (3.0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd '/content/drive/My Drive/ffrn'\n",
        "!pipreqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctDl_08XjMQC",
        "outputId": "2bc8361e-267a-491c-8fc3-fcd725173c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: Failed on file: /content/drive/MyDrive/ffrn/ff_resnet.py\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pipreqs\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pipreqs/pipreqs.py\", line 488, in main\n",
            "    init(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pipreqs/pipreqs.py\", line 418, in init\n",
            "    follow_links=follow_links)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pipreqs/pipreqs.py\", line 131, in get_all_imports\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pipreqs/pipreqs.py\", line 117, in get_all_imports\n",
            "    tree = ast.parse(contents)\n",
            "  File \"/usr/lib/python3.7/ast.py\", line 35, in parse\n",
            "    return compile(source, filename, mode, PyCF_ONLY_AST)\n",
            "  File \"<unknown>\", line 615\n",
            "    !pip install pipreqs\n",
            "    ^\n",
            "SyntaxError: invalid syntax\n"
          ]
        }
      ]
    }
  ]
}